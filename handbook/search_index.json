[["index.html", "UCSF Decision Lab Handbook Section 1 Organizing Principles", " UCSF Decision Lab Handbook 2025-09-15 Section 1 Organizing Principles The Decision Lab is a deeply interdisciplinary group investigating questions at the intersection of neuroscience and ethics, with two main streams of work: (1) understanding the neural and cognitive bases of decision-making, with an emphasis on health and financial decisions in aging and in disorders of aging, and (2) understanding the broader implications of brain diseases and new interventions for nervous system function, particularly in their ethical and societal context. We draw on a wide range of disciplines, ranging from neuroscience and psychology, to philosophy and law, to sociology and anthropology; applying both quantitative and qualitative approaches where appropriate given our questions and the current state of knowledge. This is a very broad range to cover, which highlights a critical problem of focus and prioritization that I think every lab confronts in some form. Given the wide range of available questions, where should the lab’s focus be directed and what research projects should we prioritize? And for you as individual members of the lab, how can you direct your energies and the development of your skills and training? At the broadest level, a key issue is that in science there are many questions worth investigating, and many capable research groups. If we pick out problems that everyone else is just as well-positioned to solve as we are, then the only way to succeed is to work harder and be luckier than everyone else. That’s not a sustainable path for a career or a life, and may contribute to the temptations of scientific misconduct and non-reproducible practices. So I think an overall goal for any scientist is to develop a research program (or in our case, two programs) that (1) have synergy, in that one project or question leads to understanding that can frame the next project or question, in ways that would not have been evident before what preceded; (2) draw on one’s own (or one’s team’s) unique background/skills, temperament, and interests; and (3) make use of special resources to which one has access that other’s don’t. Sometimes in business people talk about a company having a “wide moat,” meaning that it wouldn’t be easy for another company to just show up and start doing what that company has been doing. I’ve often thought that scientific research groups can also benefit from thinking about their moats. This isn’t because I view science in particularly competitive or antagonistic terms (though competition certainly does exist in science); instead, I think we’re all ultimately likely to do more good and advance understanding to a greater degree if we focus on questions that we’re particularly well-positioned to answer, as opposed to questions that could be asked by any scientist anywhere. Within the Decision Lab, this gives some focus to our interdisciplinarity. That is, while I prize and enjoy the way our lab draws upon so many fields, we do not engage in interdisciplinarity just for its own sake. There can be great value in research that is deeply rooted in the practice and intellectual history of one home discipline; and there are many examples of research that mixes work and concepts from varied fields in haphazard and superficial ways. However, given my background in neurology and philosophy, and given our strong collaborations and the especially open culture of the MAC, I do believe our lab has shown a particular ability to productively address questions in neuroscience that also involve contributions, methods, and insights from the humanities and social sciences. In keeping with what is written above, the lab will focus on such questions and try as much as possible to leave questions that are best addressed within a single discipline to other groups. For you as a lab member, if you’re developing an independent research project during your time here, I will keep asking you: does this project make effective use of methods, populations, resources and frameworks that our lab and the MAC are specially positioned to utilize? Your time here is a brief window during your career, and there are special research and learning opportunities you have now that you will not have in other places. (That hopefully, you will have chosen for other resources they have that we don’t have here.) More broadly, in preparing for your career after this lab, think about where your own particular background, skills and interests are most needed and can contribute the most, and use this to evaluate what other skills you need to pick up along the way to maximize the contribution that you hope to make. In 2018 during the lab’s first big hiring wave, Ali Zahir and I tried to articulate the essentials of lab culture that we wanted to preserve and replicate in our hiring. (So you’re all here because we thought you fit the following description.) What we decided we were looking for follows from the ideas above. While we think that diversity of experiences and perspectives is valuable for all science, it is particularly important to our work, and it is also essential that lab members value and respect such diversity. We’re looking for lab members with potential to make unique contributions to lab culture and our work–whether in neuroethics, clinical work with patients, neuroscience, computation, or logistics. And we are looking for people who, wherever they come from, have a track record of making the most of the learning and research opportunities available to them. In our work, we remain particularly mindful of our obligations to: Our funders. Most of the work in our lab is funded by the National Institutes of Health, and of course our infrastructure is provided by the University of California. This pays my salary, your salary, travel funds, research expenses, equipment, basically everything. This ultimately comes from taxes paid by everyone, and thus reflects a tremendous public investment in our work–work that should be creative, rigorous, and ethical. We strive to be prudent in our management of resources, but also open-minded in continuing to seek out special opportunities to advance science. Research participants (and their families). Our research participants, many of whom have serious, stigmatized and/or ultimately fatal neurological disorders, expose themselves to research-related risks and inconveniences in order to contribute to knowledge that benefits society. Ultimately, unproductive (unanalyzed datasets, research products that are never shared with the broader scientific community) and non-rigorous (sloppy, poorly documented, error-prone) research is unethical, as it undermines the social purpose for which participants’ effort and time are contributed. We also honor participants’ contributions by treating them with respect, by maintaining our training in clinical competence (see below) and safety, by responding promptly to emails and other inquiries, and by safeguarding the confidentiality of research records. The scientific community (particularly our readers). We aim to contribute to a broader project of understanding the brain and the ethical issues raised by neuroscience. People who read our papers should find their own understanding enhanced by our work. So we use reliable and rigorous methods, back up our work to guard against data loss, double-check our own work and other lab members’ work, identify and acknowledge our mistakes, and carefully document what we do. When we make discoveries, we help people understand them by placing them in appropriate context, avoiding undue hype, acknowledging limitations and counterarguments, and writing clearly. We use social media, our website, press and other tools to help others (including nonspecialists) find our work and so contribute to a broader public conversation. Colleagues (both in the lab and at MAC/UCSF broadly). We strive for a lab environment in which all members can be successful. Discrimination and harassment of all forms are not tolerated–please speak with me directly if you are treated in a manner that makes you uncomfortable or hinders your ability to work and learn, or observe another lab member treated in such a way. We support each other and share our knowledge. When there are disagreements or other sources of tension, we need to be able to communicate openly about them (please again come see me for help with this). We are respectful of one another’s time and that of our collaborators and research participants, so we’re punctual and reliable about meetings and appointments. That said, please do not come in to work sick. Please promote your own health and others’ by taking time to recover, making arrangements for me and your colleagues to cover for you if necessary (they’ll be happier doing so than being exposed to whatever you’ve got, and you can return the favor later), and if you absolutely need to work doing so remotely. Ourselves. Each of us should feel that we are doing work that we’re proud of on topics that we’re excited about, that we have appropriate support, learning opportunities and mentoring to take the next steps in our careers, and that we have enough space for the people and pursuits that are important to us outside of lab. We also should acknowledge that science is inherently hard (we’re literally trying to do things no one has ever done before), and that you all are in particularly vulnerable and stressful parts of your careers. Struggling is not a sign of weakness, but is also not something you’re expected to suffer through by yourself. Please come talk to me so we can strategize together about resources, about our mutual expectations, about how your work here fits into your broader career goals, and about how lab work is structured. (I’ve had many more of these conversations with lab members than you probably realize.) Nothing we do in lab is worth doing if it doesn’t also contribute to your health and happiness. "],["hours-and-expectations.html", "Section 2 Hours and Expectations", " Section 2 Hours and Expectations “Do as I say and not as I do,” but not with the usual meaning… In general, we are focused on completing the work that needs to be done, and not on observing a strict schedule. One nice thing about research is that, while many tasks such as research appointments or project meetings need to happen at specific places and times, other tasks can be more flexibly structured around personal or family obligations (dentist appointments, child or pet care, etc.) or preferences around the timing and place of work (e.g., if you prefer to work from home when you can, or are more of a morning or night person). That said, our work schedules are subject to two external constraints. The first is that research coordinators and others (particularly in union-represented positions) are subject to University-wide policy around things like work hours and overtime. The second is just that we have obligations to our funding agencies regarding the work we’ve promised to do for them, which includes that the effort you devote to those projects should map on to what we’ve allocated for your roles. At the same time, there are special research opportunities that you might want to take advantage of (e.g., for career development or other personal reasons) that are not directly tied to responsibilities on funded projects, and we wouldn’t want to unduly constrain your ability to work on those. For this reason, I make a rough distinction between people’s core responsibilities and their individual projects. Basically, “core responsibilities” are what we have allocated your funded effort to do: this is usually to work on one of our big funded projects, which have pre-specified questions and aims and that will typically extend beyond any single research coordinator’s tenure. As of 2025: we have just transitioned to our new projects Making Decisions for Other People (DfO), Dementia Experience and Caregiving (Personal Values), and Designing Devices for Human Needs (HCD Neurotech). Opportunities for coordinators to do individual original work on these projects are limited. For mentoring, it’s often valuable (but not strictly required) for people to also have independent projects, which allow them more creative room and opportunities for learning, ideally leading to first-authored posters and hopefully manuscripts. These should usually be thematically and methodologically related to, but non-overlapping with, the questions pursued in our core funded projects. For work on core responsibilities, research coordinators should generally expect to spend no more than 40 hours a week, though with some variation week-to-week. If you are regularly spending more than 40 hours a week on these responsibilities, this means that something about your role is misconceived, and please come talk to me about it. You are not required to spend these 40 hours physically on campus, and they do not all need to be during normal working hours (if you prefer to work early or late). Now, if research reasons specifically compel you to work on a weekend, or before about 8am or after 6pm (e.g., to run a participant or set up the scanner), you can and should mark this on your timesheet for overtime. (This does not apply to cases in which you simply shift the time of your work, e.g. from morning to evening, because of a personal working time preference.) Work on your individual projects is generally not eligible for overtime. (We can’t charge federal and other funding agencies for your overtime if such overtime work isn’t necessitated by the specific projects they are providing funding for.) You’re welcome to spend time during the normal working day to pursue these projects, so long as your core responsibilities are taken care of. Finally, you will notice that I will send Slack and e-mail messages on nights and weekends, and will be managing work (though at reduced intensity) when I’m on vacation. These all reflect accommodations that I’ve made in my own working schedule that maximize the time I spend with my family, but they do not represent expectations for you. So usually, e.g., if I send you a Slack message on the weekend it’s because I’ve thought of something that I’m worried I’ll forget to tell you if I wait until Monday–unless I say otherwise, I’m not expecting a response before Monday. (Sometimes questions will come up on a paper or grant deadline that can’t actually wait, but I’ll let you know and it shouldn’t be often or routine.) And while I take my work with me on vacation, I’m expecting you to actually be off Slack and e-mail while you’re away–we have enough people in lab that we should be able to make accommodations for covering your responsibilities before you leave. "],["lab-resources-overview.html", "Section 3 Lab Resources Overview 3.1 Slack 3.2 Lab website 3.3 Private Github repo 3.4 R: drive (R:\\groups\\chiong) 3.5 Box 3.6 REDCap 3.7 Open Science Framework 3.8 Other useful MAC tools 3.9 Deprecated/discouraged tools", " Section 3 Lab Resources Overview Here is a very general overview of lab-wide resources that everyone in both sides of the lab should be familiar with. (Some projects also use specific tools that should be separately documented.) As one broad principle to keep in mind: research coordinators in the lab typically stay 2-3 years, but many of our projects span a much longer period of time. We need everyone to perform protocols consistently, and to document their work carefully, so that the people who follow you can continue your work. 3.1 Slack Slack is the primary method of communication within the lab. You can customize your notification preferences (particularly important on the mobile app) so you are notified of important messages but also are not overwhelmed. Use the ‘@’ feature to get my attention if I need to get involved in a conversation, and make sure that you’re subscribed to the right channels. Key channels for everyone are #general and #random. For our new big lab projects in 2025 we have work channels set up for #decisions-for-others (a continuation of a channel previously labeled #hillblom and then #branch for online studies of decision-making in healthy older adults), #neurotech (now focused mainly on HCD Neurotech grant funded in 2024, but with some residual wrap-up work on our previous NIH-funded neuroethics grants), and #personal-values. Valerie’s been hosting a rich set of readings with discussion at #disability-studies, and a channel for #code has been used mostly for conversations about R and RStudio. More or less in the archives are channels for older lab projects: #caregivers, #dma, and #gbd. We also have channels for non-work-related conversation at #decisionzbop (a channel for music conversation), #food-opinions, #im_upset (originally started early in the COVID-19 pandemic when that topic overwhelmed all our other channels, now a general hub for concern about the state of the world), and #social for informal non-work gatherings and conversations that don’t involve me directly. As a matter of lab policy, we do not have private/locked Slack channels. Two important caveats about Slack: Slack is not considered secure or HIPAA-compliant. No PHI (protected health information) belongs there, and in general any discussion involving individual research participants should be conducted via SECURE: e-mail rather than Slack. Slack is not an archive. We can search in our lab account, but it’s not that easy to go back and reconstruct details of conversations and decisions made. So we use Slack for rapid, concurrent conversation–but anything that we might want to refer to later needs to be documented, most likely in GitHub. 3.2 Lab website Our lab website is one place where we communicate what we do and share our work with the outside world. This is hosted on our public GitHub repository, which you will need a GitHub account to edit. See the README at github.com/DecisionLabUCSF/decisionlabucsf.github.io to learn how to add your team profile. Because this is a public repository, anyone can read the code that we use to build the website, and it also has an open license inviting other investigators to use this code to design their own websites if they wish. As I say in the README, you can think of GitHub as like a supercharged Google Docs for code, allowing multiple people to work on code together, keeping track of who made what changes and when, making it easy to reverse changes that have been made, and allowing different versions to be developed at the same time. Tools like this are commonly used by software companies to maintain quality and avoid/fix bugs. We won’t be using all of the sophisticated tools in GitHub that they use, but generally I think scientists have a lot to learn from software developers: Facebook and Google can’t survive with sloppy code and neither can we. 3.3 Private Github repo In addition to our public repository, we also have a private repository that only lab members can view, which is where we document and edit our code, and maintain electronic lab notebooks. See the README at github.com/UCSFMemoryAndAging/decisionlab for details once your account is fully set up with permissions. A basic way of working with GitHub is just editing files on the GitHub website. However, to use GitHub for code that can be utilized by statistical/analytic packages like R and MATLAB, you will need to synchronize the lab repo to a folder on your computer (like a more technical version of Dropbox or Box). For more details on how to do this, see Section 9 of this lab handbook, RStudio Analysis Pathway. Here are some key initial points: As with Slack, no protected data. PIDNs and other assigned identifiers can pop up infrequently, but we should avoid other participant-level data, and especially PHI and participant identifiers (names, dates of birth, addresses, demographics, etc.) GitHub works best with text-editable files: code, markdown files, HTML, notebooks and the like. GitHub is much less useful for big files like images, Word and Powerpoint files, and they tend to clog things up for everyone else who is syncing to the repo. Images should be reduced in size whenever possible, and Microsoft Office files (Word, Powerpoint) should stay out of GitHub and go on the R: drive somewhere. Please be consistent about naming and file organization, and be obsessive about documentation and comments! Again, other people will pick your work up after you, so please be kind to them in advance. (Quick note that things can get messy if files or folders get renamed after they’re in use, so it’s good to be thoughtful and pick good names for things at the outset–see more about this in the README for the repo.) Try to keep line length to about 80 characters, and commit-pull-push! 3.4 R: drive (R:\\groups\\chiong) To recap some points from above: Slack can’t be used for protected data or discussion of individual research participants, and isn’t an archive. GitHub is an archive for code and documentation, but also shouldn’t be used for protected data or identifiers (and documentation about individual participants should be limited), and shouldn’t be used for large files such as Microsoft Office documents. So: for our quantitative projects we use the R: drive for large datasets, especially those including personal identifiers, and other useful resources that are too large to put in GitHub. (Quick personal note: I’m often working from my laptop and home computer, on which I only mount the R: drive intermittently–so if you need me to look at something that doesn’t involve PHI, it will often be a lot faster if it lives in GitHub or is temporarily available in Slack.) For instructions on how to mount the R: drive on your computer, see the Technology section of MACipedia. For using statistical packages, a model for how to utilize the R: drive with the GitHub repo is described in Section 9: RStudio Analysis Pathway). As a general overview, the pathway is: Clone the decisionlab GitHub repo to your computer. Save original dataset (e.g., a .csv or Excel file from Qualtrics or E-Prime) on the R: drive, and never write to this file again. Clean the data, using a script and a logging file that are saved in GitHub, creating a cleaned data file that is saved back on the R: drive. Analyze the data using other script and logging files that are saved in GitHub, generating new data files and graphics as necessary that are saved on the R: drive. Since everyone is using the R: drive for a variety of projects (which will continue on after you leave), keeping our group folder organized is a challenge! Please refer to Guide.docx on the R: drive which explains how the subfolders are organized. 3.5 Box In our qualitative projects we are now using Box, which is better for involving non-MAC colleagues like in MCL (Medical Cultures Lab) and non-UCSF collaborators. Note that there’s one way in which Box isn’t as good for data security as the R: drive, in that if people have Box Sync enabled then they’ll be carrying around copies of all of the data for a given project on their own laptops. (This is why drive encryption is important, but even so I’ve decided that I’m just not going to think too much about all of these copies of our data wandering around the world in people’s personal devices.) For all of our data projects, we should regularly review who including our collaborators has access to a given folder, and whether such access remains necessary or justified. 3.6 REDCap Somewhat new for 2025, we are now using REDCap for tracking participants in our studies, including enrollment and consent, and the completion of study activities. REDCap is a very useful tool though it has a bit of a learning curve, in part reflecting some advanced capabilities (used for running things like multi-site randomized clinical trials) and options that go beyond our anticipated uses. It has very good resources for securing data and tracking who should have access to which data, though we’ve heard from other researchers that coordination across site (e.g., with non-UCSF collaborators) can get messy with things like permissions. We’re still learning how to use it, though I’m enthusiastic about it and have found it to be a big improvement so far over what we were doing before. 3.7 Open Science Framework “Open science” and our lab’s relationship to this idea are discussed in detail in Section 8, Reliability and Open Science. We use OSF to host preregistrations of our studies (data collection and analytic plans that we try to write before we begin a study, and which other researchers can check against our work later on) as well as public datasets and code. To avoid concerns about access and permissions, we’ve been posting all of these materials on my own account, even when they are authored by (and credited to) other lab members. Organization in OSF is like a Russian doll, with components nested inside of larger components nested inside of still larger components. At the top level is the user’s account, which can contain a number of different projects. Inside each project could be a number of smaller subprojects, which each might represent some piece of the larger project such as the work intended to result in a particular planned publication: A key distinction in OSF is between what’s “public,” meaning anyone in the world can see it, and what’s “private.” For many projects, this might proceed in different stages, where things start out as private and we gradually open them up to public. It’s useful to note that in OSF, a private component can have public subcomponents inside of it, but a public component can’t have private subcomponents. In this schematic, there’s a private “project 1” that has three components corresponding to three planned publications that are in different stages of completion. For manuscript 1, it’s already complete and has been submitted to a journal. As the work is finished, everything is public: the preregistration, the data, and the code. For manuscript 2, we’ve already planned out the data collection and analysis plans, and so we’ve posted a public preregistration of the project that anyone can read. However, since we’re still working on it, we haven’t made the data or code public yet. For manuscript 3, we’re still planning the data collection and analysis plans, so there’s a private draft registration that hasn’t been posted and made public yet (and the data and code don’t yet exist.) Currently we have a few big “Projects” in OSF that are private, corresponding to our previous set of big lab projects listed on our website: Neurotech (in “Population survey panel studies”), DMA (“Decision-making in Alzheimer’s disease and related dementias”) and BRANCH (“Online studies of decision-making in healthy aging”). (We preregistered GBD before we came up with this organization system, so the GBD projects are scattered in their own folders.) I’ll be creating some new projects for our new 2024 grants soon. If you are writing a preregistration for the first time, consider looking at examples such as Wisdom and fluid intelligence in older adults by Cutter Lindbergh and Utilitarian moral reasoning in times of a global health crisis by Rea Antoniou. 3.8 Other useful MAC tools For these, you’ll generally need to be accessing from campus or, if off campus, signed in through the UCSF VPN (check out it.ucsf.edu/services/vpn for how to get the VPN set up). 3.8.1 LAVA and LAVA Query 5 LAVA is the MAC’s primary database for all research-related patient and participant information, e.g. demographics, visits and scheduling, research diagnoses, specimens. For our current projects, we use LAVA for the following: Getting participants’ contact information to call/email them about enrolling in one of our studies Enrolling participants under the ‘Enrollment’ and ‘Scheduling’ tabs once we have seen them for a study Utilizing the Contact Log when reaching out to participants in our studies, so that our communication is coordianted with MAC parent projects and with other subprojects Looking up participants’ most recent (and past) diagnoses for tracking enrollment numbers or running data analysis LAVA Query 5 is a (fairly) user-friendly querying tool that allows you to pull information from a selected subgroup of (or all) participants from the main LAVA dataset. You can access this on the front page of the LAVA website. If you will be running any kind of analysis, you’ll most likely use LAVA Query 5 to pull relevant participant information, download it as a csv file, and merge with your study data files. For more info how to do this, visit the LAVA Query Protocol and the Pulling LAVA Queries sections of MACipedia. It’s also useful to “ϟ Download Current Data Dictionary” from the front page of LAVA Query 5 (under Data Objects). 3.8.2 MACipedia As referenced a few places above already, MACipedia has a wealth of information on training and documentation for the MAC. Worth exploring, though can be a bit of a maze to find what you are looking for. 3.8.3 Brainsight/DASH Powerful and relatively user-friendly tools for visualization of MAC data. Includes individual-subject level graphic summaries of neuropsychological data and viewable neuroimaging data (including “raw” neuroimaging data, for which you will generally want to view the T1 images for neuroanatomy; and W-maps comparing atrophy to age- and sex-matched norms); also with some tools for performing group-level voxel-based morphometric studies that we can talk about separately if you will be running such analyses. 3.9 Deprecated/discouraged tools Excel. In general, Microsoft Excel is not an appropriate tool for data analysis or manipulations that are intended for presentation or publication. It can be used to explore datasets, but usually this is best done within a proper statistical/analytic software package such as R, Stata or Matlab. Best estimates are that between 2% and 5% of Excel formulas have uncorrected coding errors, so if you are using a worksheet with at least 35 formulas then it probably has at least one error you don’t know about (and if you’re not particularly expert in using Excel, probably many more). Field studies have shown that almost all large spreadsheets in the corporate world have multiple errors. If capitalism supposedly works by efficiently utilizing information to optimally allocate resources where they can be best used, then someone please explain to me why businesses still install this program on their employees’ desktops. In 2012, JPMorgan Chase lost $6.2 billion by relying on a risk model coded in Excel that included a simple formula calculation error. As a commentator noted about weaknesses of reliance on Excel, “There is no way to trace where your data come from, there’s no audit trail (so you can overtype numbers and not know it), and there’s no easy way to test spreadsheets.” This statement provides a decent outline of software requirements for a minimally reliable data system. We should be able to trace where the data that generate our results came from. We should have a record of changes that are made to a file, so we can diagnose if errors are introduced accidentally. (In addition, we need systems in which it’s not so easy to accidentally make changes, such as by distinguishing between viewing and editing; you ought to be able to view a record without worrying that accidentally touching your keyboard will overwrite your data.) And we should have systems in which it’s easy to validate and find out if something has gone wrong. In the past, we’ve used Excel sometimes to create or input data (e.g., for tracking participants) that is then imported to stats packages. For now, my very strong preference is to do any such work in REDCap since the potential for introduced errors is so high in Excel. Finally, some of the data tools we use (REDCap, Qualtrics [in older projects], LAVA Query) export csv and xls files–our general practice is to save them somewhere on the R: drive and avoid ever touching or rewriting to them directly, only importing from saved files into a proper stats package. Google Docs. In general, we have found that Google Docs is useful for sharing short-term projects, such as an abstract or IRB submission that needs to be edited by multiple people before submission. However, it has proven to be unwieldy for archiving/storage/documentation–we’ve noted problems with moving/deleting files, with governing permissions and file access, and in general with organization. So anything that we might want to refer back to in a matter of months or years should be appropriately organized in GitHub or the R: drive. "],["intro-to-the-memory-and-aging-center.html", "Section 4 Intro to the Memory and Aging Center 4.1 The MAC in the context of UCSF &amp; the Weill Institute 4.2 MAC clinic &amp; MAC research 4.3 MAC research “parent projects” 4.4 Learning opportunities in the MAC 4.5 Who’s who at the MAC", " Section 4 Intro to the Memory and Aging Center One challenge for new research coordinators is that the Memory and Aging Center is a huge institution with its own specific culture and organization, partly just reflecting the broader norms of academic medicine. There is a lot of helpful information on [MACipedia][MACipedia], the MAC wiki. Still, it can be tricky to figure out how all these pieces fit together, particularly given all the three-letter acronyms in use, so here is a brief orientation: 4.1 The MAC in the context of UCSF &amp; the Weill Institute In 2016, a large donation prompted a reorganization of the clinical and bench neurosciences at UCSF, under the broad umbrella of the Weill Institute for Neurosciences. This new institute, directed by Dr. Stephen Hauser, now includes: The Department of Neurology The Department of Neurological Surgery The Department of Psychiatry The Institute for Neurodegenerative Diseases The Neuroscience Graduate Program The MAC is part of the Department of Neurology, chaired by Dr. Andy Josephson. It’s one of several different divisions; other divisions that we work with a lot (particularly in the Neurotech project) are Movement Disorder/Neuromodulation and Epilepsy. 4.2 MAC clinic &amp; MAC research Our lab is part of the research program at the MAC, but when most people in the community think of the MAC, they’re usually thinking of the MAC Clinic. The clinic is located on the 2nd floor of 1651 4th Street, and provides neurological care for patients with cognitive disorders. I usually see my clinic patients on the 2nd Wednesday afternoon and the 3rd Tuesday morning of each month (also: some Monday mornings supervising a neurology resident, not open for observation). For a while we didn’t have any visitors “shadowing” in clinic, but we’ve now opened this up for people working in MAC research–let me know if you’d like to observe. MAC research appointments principally take place in the Neurosciences Clinical Research Unit in the Sandler Neurosciences Center at 675 Nelson Rising Lane. Patients as well as healthy control participants volunteer for studies to help understand the neurobiology of dementia and aging–this can include clinical trials, observational studies, neuroimaging (fMRI, structural MRI, PET, MEG), and sampling of blood and cerebrospinal fluid. Generally we anticipate that people who volunteer for studies will be comfortable with having observers, and family conferences (see below) can be valuable learning opportunities for staff; note that research participants can always decline observation by staff who aren’t directly involved in their research visit. Some patients are seen in MAC clinic and are participants in MAC research (often being referred from clinic to the research program, if they’re interested); some patients are only seen in MAC clinic and not in research (e.g., if they don’t meet inclusion criteria for our studies or aren’t interested in participating); some patients (particularly with rare conditions who are recruited from other states or regions) are only seen in research; and generally our healthy control participants are seen in research only. 4.3 MAC research “parent projects” Many of our research participants are recruited through the MAC’s longitudinal “parent projects.” These are multi-year, longitudinal research projects that follow different clinical cohorts of research participants over time. Our lab, like many others at the MAC, runs “subprojects” for which participants in the parent projects are screened and referred. Principally, we work with three of these parent projects: “PPG” (Frontotemporal Dementia: Genes, Images and Emotions program project grant) This project recruits patients with different variants of frontotemporal dementia (FTD), as well as related conditions such as corticobasal syndrome (CBS), progressive supranuclear palsy (PSP), and amyotrophic lateral sclerosis (ALS). ALLFTD (formerly ARTFL/LEFFTDS) This is a collaborative network of research centers throughout the US and Canada that see patients with frontotemporal dementia and related disorders. Part of ALLFTD focuses on familial forms of FTD, including studies of people who have genes for FTD and of their family members. Many of our participants are co-enrolled in PPG and ALLFTD, and some of the assessments they undergo are ALLFTD assessments that are common across the various ALLFTD sites. ADRC (Alzheimer’s Disease Research Center) This project recruits patients with Alzheimer’s disease, including a subgroup for: EOAD (Early-Onset Alzheimer’s Disease) - people diagnosed with Alzheimer’s Disease before the age of 65, as distinct from Late-Onset Alzheimer’s Disease (LOAD), the more typical form. Brain Aging Network for Cognitive Health (BRANCH) This project recruits healthy older adults, as determined through neuropsychological and neurocognitive assessments, for studies of regular aging. Formerly called “Hillblom” for a creepy rich guy who left UCSF money in his will, so you’ll see that name. At times, you’ll hear us discuss the “Aging Cog” (Normal Aging and Cognitive Decline) subgroup of BRANCH, which includes added assessments like MRI. 4.3.1 Our lab projects in context As of 2025, we have transitioned from one set of main projects to another. Here are the new main projects in lab, and how they are related to one another: Making Decisions for Other People In lab often referred to as “DfO,” or “Decisions for Others”; or with our grant administrators as the “K24.” This project addresses challenges in surrogate decision-making, in which a family member or other close associate must make financial or medical or personal care decisions on behalf of an incapacitated (often older) adult. People are often unprepared for this burdensome role, and prior work in cognitive science indicates systematic biases in making decisions for others. We use a REDCap database to e-mail questionnaires roughly three times per year to three groups (1) dementia caregivers, (2) healthy older adults, and (3) a matched cohort of non-caregiving healthy adults who are relatives/friends of healthy older adults in group (2). Practically all the data for this grant are in REDCap. Some materials such as the original grant proposal and our IRB and RPPR documentation are in the R: drive: R:\\groups\\chiong\\decisions-for-others\\. This project is funded by NIH grant K24AG083117 from the National Institute on Aging. In this project we recruit from the PPG, the ADRC, BRANCH, and other resources such as the Care Ecosystem project and the MAC Clinic. The SpeedType code for this grant iS MNECHW0004. Project notes for this project should be documented in the lab GitHub repo at decisions-for-others/decisions-for-others_notebook.md. Dementia Experience and Caregiving In lab generally referred to as “Personal Values”; or with our grant administrators as the “RF1.” This project draws on conceptual work by our collaborator Agnieszka Jaworska at UC Riverside to examine changes in motivation and decision-making (which we generally frame in terms of participants’ personal values) in dementia and the effects of such changes on caregivers and care relationships. Recruitment and tracking for our participants are in REDCap. Much of the data as well as materials such as the original grant proposal and our IRB and RPPR documentation are in Box. This project is funded by NIH grant RF1AG083830 from the National Institute on Aging. In this project we recruit from the PPG and the ADRC. The SpeedType code for this grant is MNECHW0005. Project notes for this project should be documented in the lab GitHub repo at personal-values/personal-values_notebook.md. Desiging Devices for Human Needs In lab often referred to as “HCD Neurotech,” as distinguished from older neurotech projects (below). This project represents an extension of our prior work documenting participant and researcher experiences in neurotechnology research, now addressing how the lived experiences of neurotechnology users can be effectively incorporated into the design of new interventions. (Drawing on disability theory and on human-centered design.) Recruitment and tracking for our participants are in REDCap. Much of the data as well as materials such as the original grant proposal and our IRB and RPPR documentation are in Box. This project is funded by NIH grant R01MH137095. The SpeedType code for this project is MNECHW0003. Project notes for this project should be documented in the lab GitHub repo at neurotech/hcd-neurotech_notebook.md. Our older projects in lab are: Decision-Making in Alzheimer’s and Related Dementias (DMA) This project studied decisional impairments in different subtypes of dementia. Guidelines for managing this project can be found in the Standard Operating Procedures on the R: drive: R:\\groups\\chiong\\DMA\\. This project was funded by NIH grant R01AG058817. Genes, Brains and Decisions (GBD) This study investigated the hypothesis that decision-making problems are a very early sign of FTD, potentially preceding a formal disease diagnosis. We compared presymptomatic mutation carriers of genes for familial FTD with their noncarrier family members, recruited through ALLFTD both at UCSF and at other sites in the ALLFTD network. Standard Operating Procedures on the R: drive: R:\\groups\\chiong\\GBD\\. This project was funded by NIH grant R01AG022983. Neuroethics in Novel Neurotechnologies (Neurotech) This study addressed ethical implications of implanted neurotechnology in patients with epilepsy, mood and movement disorders. Standard Operating Procedures on the R: drive: R:\\groups\\chiong\\Neurotech\\. This project is funded by NIH grant R01MH126997. Online Studies of Decision-Making in Healthy Aging These studies addressed decision-making an online panel of healthy older adults recruited through the BRANCH Network. Standard Operating Procedures on the R: drive: R:\\groups\\chiong\\aging-and-cognition\\. 4.4 Learning opportunities in the MAC One of the great advantages of working at an academic institution like UCSF is our access to a multitude of learning opportunities such as lectures, didactics, conferences given by world-renowned experts. A calendar including some of these events is maintained at calendar.ucsf.edu/group/memory_and_aging_center; opportunities include: MAC Grand Rounds Mondays from 10-11am (mostly in person, some hybrid) Weekly lectures, often featuring esteemed visiting scholars or other special guests. MAC didactics A two-week didactic series at the beginning of each month covering foundational topics in dementia and aging, taught by MAC faculty. These are intended for visiting students, rotating medical students/interns, fellows and new MAC staff members. Clinical-Pathological Case Conferences (CPCs) Typically about three times a year on Mondays from 10am-noon This is a dynamic and interactive experience where MAC fellows first present an interesting/unique patient case, complete with family history, social context, and clinical examination/neuropsychological findings. MAC faculty and staff are encouraged to ask questions and discuss potential diagnoses for the patient. Cases conclude with an overview of the presentation, neuroimaging, and key findings from autopsy. PPG family conferences (check MAC Family Conference Calendar) Case conferences where clinicians discuss a patient’s diagnosis based on neurological, behavioral, and cognitive data. In the first hour, the neurology fellow and neuropsycholgist (see Who’s who at the MAC, below) will present to the attending details of the patient’s background, symptoms, family/social history, physical examination, and neuropsychological testing. In the second hour, the patients and family/caregiver are invited to speak with the team, ask questions, and formulate a plan going forward. Usually held at the end of each patient’s research visit. 4.4.1 Learning opportunities within our lab In addition to these MAC-wide opportunities, our lab hosts other learning experiences: Lab didactics and journal club Second hour (if there is one) of lab meeting Usually in the fall we have a series of 7-8 didactics tailored to our lab’s specific research interests, methods and practices, with the aim of establishing common understanding across different new lab members with various backgrounds and interests. After completing this series, we’ll have a journal club: lab members will prepare a brief presentation on a journal article of interest to them, connected to our lab’s current research projects. Another great use for this time is to present work in progress, such as a manuscript or poster that someone is working on. Some weeks we’ll skip if I have a conflicting clinic or committee meeting. Mentoring meetings Wednedsays from 10-10:30am (flexible) Rotates across lab members from week to week; designated time to discuss job performance, team issues/concerns, career opportunities, applications to grad/professional schools, or other topics important to you. (Code meeting). Currently on pause, previously a meeting on Thursday mornings to discuss issues in computational code. Finally, technically not within our lab or MAC, but several people in our lab are helping to organize events through UCSF Bioethics: Clinical ethics journal club (details at tiny.ucsf.edu/ClinEthicsJC) 4th Thursdays of each month from 4-5pm A meeting aimed at clinicians in the 5 UCSF-affiliated Bay Area hospitals, to discuss a recent paper relevant to the ethics of clinical practice and research. Currently run over Zoom, let Chanel Matney know if you’d like to be added to the mailing list. Grand Rounds (scheduling variable) These will be meetings coordinated with other clinical departments or research institutes at UCSF, to address ethical and societal questions arising in these specific domains. 4.5 Who’s who at the MAC You’ll be interacting with a lot of different people in different roles. Not comprehensive, but here are some of the titles people will have and how they’re related: The boss Dr. Bruce Miller, MD, behavioral neurologist and Director of the MAC “Attendings” This is a medical education term for the supervising physician on a clinical team. These are MDs who’ve completed training in a clinical specialty (mostly behavioral neurology, though Victor Valcour is a geriatrician and Mary De May is a psychiatrist); they’re responsible for the fellows’ and/or any students’ work in research and clinical visits. All are faculty; some are also researchers who run their own labs/research programs, while some are purely clinical faculty. Neuropsychology faculty Psychologists (PhDs or PsyDs, distinct from MD-psychiatrists) with expertise in neuropsychological testing, who’ve completed training. Some are researchers who run their own labs/research programs, while some are purely clinical faculty. Nurses, social workers, genetic counselors I’d encourage getting to know these other clinicians–some are based in the research program, and others are based in the clinic. They’re great resources for understanding clinical issues, practical tools, community resources, and patient/family concerns. BNTP fellows Mostly US-trained MDs who’ve completed medical school and residency training (usually in neurology, sometimes in geriatrics, psychiatry or other fields) and are pursuing further training in behavioral neurology. You’ll see them presenting at PPG/ADRC case conferences and at CPCs. GBHI Atlantic Fellows for Equity in Brain Health An international cohort of scholars from various disciplines and many different countries, hosted at UCSF and Trinity College Dublin. Some who have clinical (MD or neuropsychology) backgrounds will occupy clinical research roles analogous to BNTP fellows or neuropsychology trainees. Neuropsychology trainees People who either have finished their clinical doctorate (PhD or PsyD), or are in the clinical training portion of their doctorate, involved in neuropsychological testing of clinic patients and research participants. Postdocs Researchers who have completed graduate-level doctoral (usually PhD) training, engaged in research here usually for between 2-4 years before seeking a permanent academic or industry research position. Research coordinators Most of you–research staff engaged in documentation, participant recruitment/tracking, administering tasks/measures, and many other things necessary to keep the place running. One of the great strengths of the MAC is its robust RC community, with many people who are preparing to go to graduate or professional schools, or are deliberating about next steps in their careers. "],["guidelines-for-clinical-interactions.html", "Section 5 Guidelines for Clinical Interactions 5.1 MAC/MACipedia resources re: clinical encounters", " Section 5 Guidelines for Clinical Interactions Originally from Ali Zahir and Madhu Manivannan Our work is centered around learning about and caring for older adults and patients with neurological disorders. As such, it is extremely important to us that we treat our research participants with respect, kindness, and understanding. After all, they are voluntarily devoting their time to help us, often with little to gain for themselves. These guidelines are by no means exhaustive, and are meant to highlight some considerations to take into account when interacting with our unique patient population. Please make sure to use person-centered language. (i.e. refer to our participants as patient with bvFTD as opposed to the “she’s a bvFTD”). In general, “person-with-x” is preferred to “x person”: i.e., “person with dementia” vs. “demented patient,” and similarly for terms such as “diabetic,” “epileptic,” etc. When greeting the patient: Introduce yourself, shake their hand, look them in the eyes, and treat them with the dignity they deserve. Acknowledge that they are providing us their time and energy by participating in our study. Our patients endure some rather grueling days and we want them to understand we appreciate them. Walk at their pace. At times, patients (or even healthy controls) can have a slow gait and speed and therefore you may need to slow your pace. In the room, allow the patient to be seated by the door. We don’t want them to feel trapped in our exam rooms. When obtaining consent from a patient, make sure to speak slowly and clearly. Avoid acronyms, esoteric terms, and jargon. Make the information in the consent form digestible for them. It helps to have a script prepared in your head that covers the following: Purpose of the study/what our lab is interested in What participation entails Risks and Benefits Rights as a participant Provide opportunity for questions Similarly, when explaining a task, please speak slowly. Pause frequently for questions and always check-in to see if the patient understands what you are saying. Always provide a patient enough space; do not dominate the interaction and allow them time to speak. Listen with intent. Make sure to listen to what the patient is saying, rather than focusing only on what you want the patient to do. Provide affirmations and reflect back to the patient what they have said so that they know you are listening. When interacting with a patient and their caregiver, be sure to remain neutral. You do not want to side with one party. If there is a disagreement between the caregiver and patient, do not interfere but make sure you are physically on the side of the patient. I’ve learned this allows the patient to not feel alienated. As you take patients through the task, be mindful of their capabilities and frustrations. If they appear agitated, uncomfortable, overly exhausted, or confused, pause the task and remind them that they can stop at any time. Be sensitive to their needs. Our tasks can be cognitively taxing, so it is important to continuously monitor the patient’s comfort level. Patients will sometimes directly tell you they do not want to complete the task. Assure them that this is perfectly fine, and thank them for their time and their willingness to come in for this research visit. Some patients will apologize for not understanding the task, asking lots of questions, or not being able to perform well on the task. Take the time to listen to their concerns, and sympathize (e.g. if a patient is frustrated at not being able to come up with words for a memory task, you might respond by acknowledging their difficulties and encouraging them on their performance). Don’t be condescending or patronizing–when in doubt, listen to what patients are saying. Sometimes that’s better than responding or trying to provide solutions. Always thank them for participating in our research–this is completely voluntary and our patients travel long distances and undergo hours of exhausting tasks and scans to help us with our research! 5.1 MAC/MACipedia resources re: clinical encounters “What is Dementia?” (with discussion of subtypes) Consenting Capacity to Consent to Research, see also my article with Neil Vaishnav Dealing with Difficult Patients MAC Suicidality Protocols Responding to inquiries regarding Physician Aid-in-Dying "],["a-brief-guide-to-the-scientific-literature.html", "Section 6 A Brief Guide to the Scientific Literature 6.1 Getting started on a topic 6.2 Reading in more depth 6.3 Using RSS feeds to follow the scientific literature", " Section 6 A Brief Guide to the Scientific Literature A few notes on the scientific literature: In our lab, like in any lab, we’re trying to advance scientific understanding, adding to what is already known. So, we have to keep track of the scientific literature and what actually is known, to help us think about important next questions that haven’t been asked and to place our findings in appropriate context. There is a lot of literature–arguably too much. You can’t read it all, and unfortunately just reading as much as you can is not as helpful as one might hope. Among other things, as one highly influential paper has argued (I think persuasively), most published research findings are false. (Ioannidis, 2005; see more on this in Section 8, Reliability and Open Science) So, we need to read widely, but also be critical of what we read. If you’re reading an older paper, are there more recent follow-up papers (either by the original paper’s authors or other scholars) in which the key finding is replicated and then built upon? My favorite example (and the basis of my own first experimental paper) is Greene JD, et al, “An fMRI investigation of emotional engagement in moral judgment.” It turns out that Greene’s title and interpretation are wrong (he found that a particular type of moral reasoning activates the default-mode network, which isn’t particularly connected to emotional engagement), but the association of DMN activation with this type of moral reasoning has been very widely replicated in all sorts of follow-up studies. If you’re reading a newer paper, one thing that many scientists don’t understand well is prior probability (see Ioannidis)–that is, even before doing some experiment, how likely is that study’s hypothesis to be true given what we previously knew about the topic? For instance, do we have reliable evidence regarding related “in the neighborhood” hypotheses or constructs? Is there a plausible mechanism linking the predictor and outcome that coheres with other things we know about the system under study? Can we be confident that the predictor and outcome can be measured reliably, or should we expect lots of random (noise) variation in measurements? (If the latter, then measurable effects will be small and large samples are needed.) Are the reasonably expected effect sizes large in relationship to the expected measurement error (e.g., brain structural differences between patients with dementia and controls) or very small or particularly subtle (e.g., interaction effects among two unvalidated measures in a study of individual differences within a relatively homogenous sample of healthy subjects)? Even if a paper shows that A is related to B with a very small p value, if that finding isn’t connected to any prior knowledge we have about A and B, or if there isn’t some plausible mechanism linking A and B, or if A and/or B are assessed using measures that are noisy compared to the effect size but the samples are small, the finding is probably false. Here are some other rough and imprecise factors to consider when evaluating a study: First, while preregistration isn’t a guarantee (and some papers reporting preregistered studies don’t actually follow the protocol described in the preregistration), it does protect against some major sources of false positives and is a positive indicator that the authors are concerned about reliability. Same goes for authors who share their data, which is increasingly expected by good journals. Finally, impact factors and other measures of journals’ reputations are flawed, but can be helpful to think about, particularly if you’re not yet confident in your own assessment of scientific quality and reliability. Very high impact factors do not by themselves indicate reliability (and papers in the “highest impact” journals may actually be less likely to replicate…), but very low impact factor journals may be “predatory journals” or in other ways unconcerned with scientific best practices. 6.1 Getting started on a topic Many scientists would disagree with me, but if you’re completely new to a topic area in science, I wouldn’t recommend that you start by picking up a bunch of experimental papers in that area to read. After all, most research findings are false (see above), and you’re not going to be well-positioned to sort out which are which. I think it’s better to start by trying to get a conceptual framework for the topic–this might turn out to be wrong, but at least is a way of fitting empirical findings together, and can then guide your intuition about whether a reported empirical finding actually fits with whatever else is known (or believed) in that domain. So, I would recommend starting with at least two or three high-quality review articles on a topic, preferably from within the last 5-7 years and from different authors/groups. Two journal series that I like for review articles are Nature Reviews (e.g., Nature Reviews Neuroscience, Nature Reviews Neurology) and Annual Review (e.g., Annual Review of Neuroscience, Annual Review of Psychology), and you can also find helpful reviews in other high/medium-impact journals in a given discipline. Here are some options for searching for a review article in PubMed: For our search filters here we’ve set “Article types” to “Review,” we’ve restricted our search to the last 5 years (you can play with this), and we’re sorting by “Best Match.” This has yielded hits in Nature Reviews Neuroscience and the Annual Review of Clinical Psychology (a little further down), among others. Keep playing around with your search terms and filters, and do some scanning of these papers, to find the reviews most relevant to your interests. Of course, when you start reading, you’ll find that these reviews are subject to different forms of bias. Review paper authors are often influential figures in the field, and tend to cite their own findings–in part because many readers will be particularly interested in a prominent scientist’s interpretation of their own influential work (more than of someone else’s work), and also because people usually study and write about what they think is important, so will tend to think things that they have studied and written about are important. So this is something to be aware of, and also a reason to look for reviews by different authors/groups. 6.2 Reading in more depth Next steps for reading depend a bit on your purposes, but often begin with identifying a key paper, author or group/lab of interest. For instance, in reading reviews you might see repeated references to a highly influential paper, or you might be working on a task in our lab that we’ve adapted from another group. Then you may want to read other things that these authors have written about the topic or task, and use the references in those articles to find other papers of interest. If you have a key paper that you’d like to use as a basis for further searching, PubMed and Google Scholar both have options for finding “Similar articles” and “Cited by” articles (that is, articles that have cited your key paper as a reference). Another resource that’s helpful, particularly when you’re further along (or are actually drafting a paper), is JANE (Journal/Author Name Estimator), also discussed in Section 7, Choosing a journal. Here you can enter an abstract (either from a target paper or for a manuscript you might be drafting) and find journals, authors and articles that match as related to this entry. 6.3 Using RSS feeds to follow the scientific literature This is the method I’ve been trying to use since 2021, will see if I can stick to it. Here I’m borrowing from the Fraser lab–see there for more details about what RSS is. Basically, every time a journal updates its content, it will also update the RSS feed. So this way, you don’t have to keep randomly checking websites for updates, you’ll know automatically when there’s something new for you to read. First, go to feedly.com and create a Feedly account. Feedly is an RSS aggregator–it puts all your RSS feeds in one place, and has handy iOS and Android apps that you can use to check your feeds on the go. Next, add feeds for your journals of interest to Feedly. If you click the “+” (plus) symbol in Feedly for “Follow New Sources,” you can search for journal titles in the search bar, or you can go to the journal website, track down the RSS link location and copy it to the bar. Finally, you can add custom PubMed searches. I have set up PubMed RSS feeds to follow key scientists, friends and collaborators so I can know when they publish new papers. Go to pubmed.gov (IMPORTANT: for this, don’t use pubmed.ucsf.edu) and search for an author using their full first name and last name (plus middle initial, if they have a common last name and use their middle initial reliably), followed by [fau] (for full name author search). After searching, click on “Create RSS” which will drop down and show some options for your feed (not too important to worry about for our purposes). Click “Create RSS” and a URL will be created for your feed. Click “Copy” and then paste the feed URL address into the Follow New Sources bar in Feedly. If you want you can do even more complicated searches and can use other field codes, e.g. [tiab] for title/abstract, or [jo] for journal name. For example, if you just search for “framing effects” as a topic you’ll get lots of articles on health promotion strategies that aren’t directly relevant to us. So instead, I have a saved search for &quot;framing effect&quot; AND (psychology OR neuroscience OR neuroimaging) that excludes a lot of articles not of interest. I have another saved search for articles about decision-making in patients with dementia or MCI: (dementia OR alzheimer* OR aging OR &quot;mild cognitive impairment&quot;) AND (&quot;decision neuroscience&quot;[tiab] OR neuroeconomics OR ((neuroscience[tiab] OR neuroanatom*[tiab] OR neuroim*[tiab]) AND (&quot;decision making&quot;[tiab] OR &quot;decision-making&quot;[tiab] OR financial[tiab]))) "],["authorship.html", "Section 7 Authorship 7.1 Why it matters, and what it means 7.2 Tips for writing papers 7.3 More about figures 7.4 Notes on posters", " Section 7 Authorship One of the general aims of the lab (and often a personal aim for research coordinators and students) is to get members involved in authoring papers and posters. (Note: much of what follows primarily concerns authorship of papers. Because posters tend to be reviewed less selectively and in biomedical fields are often seen as leading up to papers, standards for authorship for posters tend to be less stringent and less contested. Though there can also be disciplinary differences: such generalizations about the relative importance of conference abstract authorship in biomedicine might not apply to fields like engineering where such abstracts are more competitive.) 7.1 Why it matters, and what it means One source of ethical problems with authorship is that it serves several functions in science and academia, and these multiple roles can give rise to conflict and misunderstandings–both within research groups, and from one group to another. Among other things, we use authorship for: Establishing credit for work. This is perhaps the key evidence used for promotions and faculty appointments, and is often a critical component of applications to graduate or professional school, residencies, postdocs, etc. Proof of productive collaboration. This can be important, e.g., if our lab is planning to apply for a grant application with another lab. Having existing papers with authors from both groups would show a track record of working together. Responsibility for the integrity of work. In some ways, this might be the most important yet most overlooked aspect of authorship. Authors are each responsible for ensuring that work is conducted rigorously and reported accurately. Assignment of authorship is relatedly important for detecting conflicts of interest in a piece of work. This can lead to several unfortunately common problems. Ghost authorship is when someone who authored or co-authored a paper is not listed as an author; e.g., if a pharma company scientist drafts a paper, which is then published under the name of a prominent academic. Courtesy authorship is when someone who didn’t author a paper is listed as an author; e.g., if someone adds their department chair as an author to curry favor with them. Relatedly, sometimes people will include an author’s name without permission, either due to sloppiness or in hopes that a paper will be reviewed more favorably if a prominent scientist is listed as an author. This is a problem because authors are all responsible for the integrity of a paper. Criteria for authorship vary across disciplines, and sometimes across journals in the same discipline. As a common starting point for discussion, we use the ICMJE guidelines applied by medical journals because they’re the most explicitly codified. In outline, the 4 criteria that must all be met for authorship are: A. Intellectual contribution B. Drafting or critically revising the paper C. Approval of the final draft D. Accountability (if the integrity of the work is challenged) Many problems in authorship come from discrepancies in the contributions that people make to (A) and (B). For example, someone who helps to initiate a project but then leaves might make major contributions to the design of the work, but if this person is not involved in drafting or revising the paper would not count as an author. Because of this, it’s very important that before writing begins on a paper, consideration should be given to (1) who has made significant intellectual contributions to be eligible for authorship, and (2) whether or how these people can be given a reasonable opportunity to contribute to drafting or revising the manuscript in order to count as authors. If you are planning to be 1st author on a paper (see below), please consult the MAC Authorship Guidelines when you are conceiving the project and as you are drafting the paper, to ensure that you’re not leaving out someone who should be eligible for authorship. 7.1.1 Authorship order The author positions that have established meanings across biomedical institutions are first, second, and last author. The first author is usually the person who initiated a project and did most of the work, including organizing the other authors and managing the submission process. The last author should be the mentor or supervisor, and is responsible for “big picture” issues–placing findings in context, targeting a journal, etc. The person who did the next most to complete the work is the second author, and pretty much everyone else is a “middle author.” Sometimes you’ll see two people listed as “co-first authors.” Since inevitably one person’s name does have to come before the other’s, credit doesn’t really get shared equally, so in most cases we should just make an active decision about who is first and who is second. 7.1.2 Involvement of RCs and students Involvement of more junior members of the research team as authors requires planning in advance, to ensure that they do make sufficient intellectual contributions and play a significant enough role in drafting the manuscript to count as authors. For example, while senior colleagues may plausibly contribute to (B) by drawing on their past scientific and publishing expertise to critically revise a completed draft, it is less plausible that junior colleagues are in a position to contribute in this way. Involvement of more junior colleagues requires planning at the outset of drafting. In my experience, the most feasible drafting roles for RCs and students include: writing parts of the Methods and Results, creating figures/tables, and organizing references. Reserving a paragraph or a figure for a junior colleague to contribute, even if it’s something the first author could have done, can be a way of ensuring that RCs or students who deserve credit for their intellectual contributions (e.g., of acquiring or analyzing data) also fulfill drafting criteria for authorship. If you are either an RC or student and are interested in becoming the first author of a paper, we will meet to discuss what this means for you in more detail. As a rough outline (see more below), papers for scientific/medical journals usually have the structure: Introduction Methods Results Discussion In my view, if I’m senior author I should expect to have more responsibility for the Introduction and Discussion, since these often involve putting our findings in a broader context. So while I expect you to have principal responsibility for the Methods, Results, and any figures or tables (including organizing the contributions of middle authors), I don’t expect too much from you in the Introduction and Discussion. It’s often a worthwhile exercise to have you take a first stab at drafting them, but in my experience I almost always rewrite them when RCs or students are first authors. So: don’t agonize too much over them–writer’s block over the Introduction and Discussion should never be the cause of delays in completing a manuscript draft. 7.2 Tips for writing papers Many people find writing intimidating. Some resources to get you started include: Strunk &amp; White, The Elements of Style. A classic of English composition, emphasizing clarity and brevity as services to the reader. Not every rule in here needs to be followed strictly, but if you understand the rules you’ll be in a better position to break them when needed. Warren Browner, Publishing and Presenting Clinical Research. More specific to medical/clinical research, walking you through the structure of papers by section (since much of this structure is shared with other scientific papers, this resource may be helpful even for non-clinical research). Mensh &amp; Kording, “Ten simple rules for structuring papers.” PLoS Computational Biology 2017. Addressed more to non-clinical scientific research, but again I think broadly helpful. In R:\\groups\\chiong\\Resources\\Papers-methods-and-reliability. Gernsbacher, “Writing empirical articles.” Advances in Methods and Practices in Psychological Science, 2018. In R:\\groups\\chiong\\Resources\\Papers-methods-and-reliability. 7.2.1 Choosing a journal This has gotten more complicated in recent years, in part because of the phenomenon of predatory journals. These are journals that often have names very similar to highly-respected journals, but that don’t actually exercise peer review or any real editorial oversight. These predatory journals often use a version of the “open-access” model–while legitimate open-access journals (like the PLoS journals) charge the authors a fee to cover the costs of editing so that readers can access papers for free, predatory open-access journals accept every paper sent to them so that they can collect fees from authors without actually providing editorial services. If you’re not already familiar with the journals in a given domain, it’s probably worthwhile to use the Journal Impact Factor as a rough guide to journals’ reputation. Impact factors are pretty flawed measures of journal quality (think of them like U.S. News college rankings), but they can give you a general impression of journals’ reputations and at least hopefully help distinguish predatory journals from similar-sounding legitimate journals. Another great tool is JANE (Journal/Author Name Estimator). This is a tool where you can input your title and/or abstract, and it will suggest journals (including rough impact factors) based on keyword and textual similarity. You can also use this tool to find articles and authors likely to be relevant to your work. 7.2.2 Title Most good scientific papers have a straightforward, simple finding–usually either of the form A &gt; B or A ~ B–which should similarly arise from a straightforward question. For your paper, you want your potential audience to quickly grasp your research question/finding so they can figure out whether it’s relevant to their interests. (And then hopefully read it!) It’s great if titles can include the Predictor, the Outcome, and the Population; e.g., Han and colleagues, “Financial literacy is associated with white matter integrity in old age.” Someone who is totally naive about this literature can still figure out the finding and probably can infer the research question. An expert in this literature can also figure out the motivation and relevance of the finding, and probably can also figure out much of the methods (a financial literacy questionnaire for the predictor, diffusion tensor imaging (DTI) for the outcome, in a healthy older population). 7.2.3 Introduction (present tense for what is currently known) If your finding takes the form A &gt; B or A ~ B, your introduction should show why it would matter to know that A &gt; B or A ~ B. Usually in 3-4 elegant paragraphs (except for psychology journals, those introductions are crazy long) you want to: establish the scientific, clinical or public health importance of the topic briefly summarize previous research in this area identify shortcomings or gaps in current knowledge (focused on problems that your study will fix) show how your study addresses these shortcomings or gaps (sketching an overview of your hypothesis, design, sample and methods) Basically, in the introduction you want to get your audience interested in the question that your study is set up to answer. One classic logical organization for a scientific introduction, which I think most scientists actually do quite poorly, is the “funnel.” Understanding the logic of the funnel will go a long way in helping you design impactful studies. Recall that your study is only a tiny contribution (maybe the work of a handful of people over a couple years, studying a few hundred subjects in one area) to a vast accumulated scientific literature. How is this little study going to move the field forward or even just change how your reader thinks about the topic? If you use the funnel organization, you start out with a really big problem that many people care about, and gradually show that the answer to this big problem depends on smaller questions: how to measure A, how B is connected to C, how to deal with inconsistent findings between study D and study E. So while the funnel begins quite wide, as the argument continues the reader can see that a critical part of this wide question is the narrow question at the end of the funnel. Ideally it’s this pivotal, small question that unlocks the various problems set up in the introduction that your manuscript is prepared to answer. 7.2.4 Methods (past tense for what you did) Now demonstrate to the reader how you plan to show that A &gt; B or A ~ B. Note that if the study has a preregistered protocol, it’s best to try to keep the description of the methods similar to the description in the protocol–when feasible, don’t even paraphrase but instead keep the exact same wording. Key overall elements of the Method are: In clinical research, specifying the type of design (e.g., retrospective vs.  prospective, case-control vs. cohort vs. RCT) Research subjects: the population, the inclusion/exclusion criteria, the controls Measurements, both for predictors and outcome variables. It’s important to organize these in a way that’s friendly to the reader. Usually it’s most straightforward to group the predictor variables together and then the outcome variables. Also see Browner for thoughts about how to convey the appropriate level of detail for your target audience. Analysis: your statistical plan and why you used the tests that you used. Make sure you explain decisions you made that might seem odd to reviewers (e.g., excluding subjects and your rationale for doing so), and any missing data and how missing data were handled. 7.2.5 Results (past tense for what you found) A tricky thing for new scientific writers is understanding the relationship, on one hand, between the Methods and Results; and on the other hand, between the Results and the Discussion. “All results must have methods; all methods must have results.” Basically, if you describe some procedure in your Methods, then whatever happened needs to show up in your Results. Similarly, if you describe some finding in your Results, your reviewers need to be able to figure out how you arrived at that finding from your Methods. In general, the Results are supposed to describe what you found, and the Discussion is supposed to interpret them. So the Results are not supposed to include any interpretation–sometimes you can get away with something like, “As predicted, A &gt; B,” but some reviewers will balk even at that. Also, there should be no new results in the Discussion. It’s really important to structure your results so that your main or most interesting finding is obvious and easy to find. You don’t want it buried in the middle of a long paragraph or hugely complicated table or figure. The normal order is to start with a quick rundown of descriptive results (who your research participants were and what happened to them), and then get to your main analytic finding. Usually you want to keep your strongest findings first, and simple results (like main effects) before complex ones (interaction effects, multivariate models, validity checks); it’s also helpful when possible to mirror the ordering of your Methods. One especially tricky thing is figuring out which results just to describe in the main text, as opposed to creating a figure or table. See below for more on figures. Usually if your result is just a matter of one or two numbers (e.g., your finding is A &gt; B and you want to show A and B’s point estimates and confidence intervals), then you should just put this in text. Overall, the text and the tables/figures should complement one another. As Browner (Chapter 5) notes, new investigators often write the text around their tables and figures, e.g., “Table 1 shows… The multivariate results can be seen in Table 2…” See Browner for suggestions on how to use the text to help your reader interpret the data in your figures and tables. 7.2.6 Discussion (past tense for talking about what you found, present tense for what it means) It’s helpful to go back to the question that you posed in the Introduction, and then to indicate that your study has answered this question. Synthesize and summarize the key findings, including particular mention of any that are surprising or otherwise noteworthy. Try to use words rather than numbers to do this; if numbers are included they should be kept simple. Then write about what you think the results mean, indicating how strongly you believe them. Some people treat the Discussion as a “sales job”–I don’t think this is a good way to view your relationship to the reader. My preference is to try to perform a service to the reader, by helping the reader to place your findings in appropriate context. So if you think your findings really do “demonstrate” that A &gt; B or A ~ B, go ahead and say so; but if you know that the data admit of alternative interpretations, you can signal this by saying that your findings “suggest” that A &gt; B, or that a reasonable interpretation is that A &gt; B. In some sense, a good Discussion can parallel a good Introduction; while the Introduction is a “funnel” that zooms in from a wide question to the narrow focus of the individual study, the Discussion turns this into an “hourglass” and zooms us back out to the big questions we started with. (But Discussions are generally formatted more loosely than Introductions, so don’t work too hard to make the “hourglass” idea fit.) If there are special features of your study you might choose to highlight them, but avoid bragging or claims of priority (“this is the first/largest…”). Briefly synthesize previous research to show how your work relates to the existing literature. Set up your limitations and explain them (e.g., why you used the sample you did, or why a given statistical model didn’t fit), including not only design/methodological limitations, but also limitations of your interpretation of findings. 7.2.7 Person-centered language and other terminology As with our Guidelines for Clinical Interactions, we aim to use person-centered language and appropriate terminology, particularly when writing about older adults and people with dementia, The American Geriatrics Society has some helpful guidelines on this (scroll down to “JOURNAL STYLE”). Overall, the preferred term for someone with a disease, e.g. Alzheimer’s disease, is “person/people with Alzheimer’s disease.” We should be cautious about describing someone as a “patient” (e.g., “patient with Alzheimer’s disease”) unless they’re specifically being discussed in a medical context or in the context of a patient-clinician relationship. In a research context, terms like “participant with dementia” may also be used. Also consider “care recipient” and “caregiver” if the paper specifically discusses someone in the context of a caregiving relationship. At nearly all costs we avoid referring to people by using acronyms (e.g., “ADs,” “bvFTDs,” “PWDs” [for person with dementia]). Finally, for people writing about caregiver/care recipient relationships, there’s been increasing attention to appropriate language for caregiving. One useful resource is an editorial in the Journal of the American Geriatrics Society, Words Matter: The Language of Family Caregiving. The key point is to avoid the terms “informal” and “formal” caregivers for caregivers who are unpaid and paid, respectively. See Table 1 in the paper for more detailed recommendations. 7.3 More about figures For an overall guide to how to communicate statistics with figures, I strongly recommend Best Practices for Data Visualization, a guide by the Royal Statistical Society but very broadly relevant. Another resource you might consider consulting is Rougier, Droettboom &amp; Bourne, Ten simple rules for better figures, PLoS Computational Biology 2014. One important thing to understand about figures is the difference between vector and raster graphics. Raster graphics are the most familiar to people–these are the images created by digital cameras, and are composed of a grid of tiny squares called pixels (similar to needlepoint). To edit a raster image, we use programs like Paint or Photoshop to change the colors assigned to each pixel. Vector graphics are instead made up of lines and curves that are mathematically defined. (If you’ve used the drawing tools in Powerpoint or Google Slides, these are vector graphics.) To edit a vector image, we use programs like Illustrator or Affinity Designer to move or change the points that define the lines and curves. Because raster graphics are actually made up of tiny squares, there is a limit to how far you can magnify them–zoom in too close, and you get jagged effects where what is supposed to be a curve or slanted line turns out actually to be blocky. Meanwhile, vector graphics are defined mathematically, so you can zoom in on them indefinitely. Related to this, it’s very easy to convert vector graphics to raster graphics, but it’s hard to go in the other direction. Most figures, like line or bar graphs created by Stata or R, are produced as vector graphics. These should be saved in an appropriate vector format (most commonly EPS, sometimes AI), and edited as vector graphics. If you convert them to raster, then it will be nearly impossible to edit them again as vector graphics. Some other figures (such as from neuroimaging) are generated as raster graphics. When editing these, please be aware that the screen resolution of your monitor (often something like 72 dots per inch) can be much lower than print resolution (300-600 dpi). If your graphics are intended for publication, they need to be created and edited in print resolution and you should check that they look good when viewed close-up. 7.3.1 Style and design As emphasized by Rougier and colleagues, do not just use the defaults from the stats program (Stata, R, Matlab…) used to create your figures. Instead, you want your figures to demonstrate your active thinking about the most revealing and helpful ways to display your data–e.g., if there are multiple colors, this should be because the colors denote something meaningful or informative that you’re signaling your reader to pay attention to, and not just because the colors are part of the default settings on your stats package. You have two main options for customizing your figures. First, you can modify output settings of your stats program, and in this way tweak your labels, legends, line thickness and color, point markers and so forth. Second, you can save the figure in an editable vector format such as EPS, and then open the image file in a vector editing program such as Illustrator or Affinity Designer. In practice, I usually rely on a combination of the two approaches, as some changes are easier and more efficient to do in a stats program, and others are easier to do in an editing program. For consistency, I recommend changing all figure labels to Helvetica, unless another font is specified by the journal you’re submitting to. (Stylistic consistency will help when we want to combine figures from different projects, e.g. in future grant applications or presentations. In general, our lab products, following UCSF style, use the fonts Helvetica and Garamond. In a pinch you can use Arial in place of Helvetica, but I will complain about it.) See the website-and-identity/fonts folder in the R: drive. Finally, when possible it’s nice to use the UCSF color scheme. Just using any consistent color scheme will help later on when we want to combine figures from different projects, and relying on the UCSF scheme will help things look consistent on posters. To access the UCSF color scheme, go to the UCSF brand guide for colors. You’ll notice that each color has numbers associated for RGB (quantities of red, green and blue light). To use these in your program, you need to go beyond the preset colors–look for a color palette or an option for More Colors… Then from that find the option to enter the numeric RGB values directly (rather than trying to choose a shade that matches), and use the values from the UCSF color scheme: 7.4 Notes on posters I like to think of posters as preparation for writing the paper that will eventually result from your project. Selfishly, in the poster session you’ll want to talk to people who know enough about your topic or methods to be helpful in your thinking about how to write up the project–or, maybe if you’re lucky, to PIs or other people whom you’re trying to impress. Communicating your findings to other people who are not in the area is rewarding but of secondary importance. Trying to view posters in poster sessions is cognitively overwhelming. Some conferences like SfN have thousands of posters. Personally, I’ve found that I can only take in a handful of posters in a session, and I actively ignore everything else in the poster hall because otherwise details get jumbled together and I don’t remember anything. When you’re designing your poster, think about ways to specifically draw in the people you want to talk to. Depending on what stage you’re at in your project, you might be looking particularly for feedback on your framework, or methods, or analysis, or findings. Whatever it is, you want it to be obvious from a distance to people who are interested in that area that your poster is relevant to them. E.g., use eye-catching figures, but only if those figures quickly communicate what your project is about, and particularly the parts that you’re looking for feedback about. And knowing that people are cognitively overtaxed, don’t make them work to figure out what you mean: use labels and headings to help people know what they’re looking at and what you want them to think. Here’s a mockup of a poster (not the actual one–don’t examine too closely) that demonstrates some strategies for organizing a poster: High-value material is placed at the top of each panel: on the left, the conceptual framework and motivation; in the middle, the major figure; and on the right, the takeaways. This makes these elements easier to spot from a distance in a crowded hall. Finer details are placed at the bottom: these are things you’re including for thoroughness and might be of interest to someone who’s already interested in the poster, but aren’t things that you expect to draw people in. Here I’ve put background info on the study from which the data is derived, a table characterizing the participants (usually “Table 1,” but here crammed at the bottom and in tiny font), and Acknowledgments/References. The poster is organized to make it easy for people to visually scan and find the parts they’re interested in. There’s a fair amount of white space, so the text isn’t all crowded on top of itself. (Even though this required us to make the font size a little smaller than it might otherwise have been.) The headers use a consistent font/color convention and a familiar Intro-Methods-Results-Discussion format. When someone is reading a paper, it’s the only thing they’re looking at, so you don’t always have to spell everything out for them. When someone is looking at a poster, it’s one thing among many others in a crowded and distracting poster hall, so you have to make it very obvious what lessons they’re supposed to draw or they’ll get frustrated and move on. Here, each figure in the Results section has a descriptive title, and also includes brief text to help viewers know what’s going on. Similarly with the takeaways: try to come up with two or three really quick and clear messages that viewers should walk away with at the end. Note that the graphics use the old UCSF color scheme; the idea was to make them easy to combine from different sources without clashing. (The “Care Ecosystem study” diagram is taken directly from Kate Possin’s paper about the Care Ecosystem.) The old color scheme doesn’t differ too much from the new one, but we should try to use the new color scheme going forward. One last picky thing. The Windows versions of Microsoft Word and Powerpoint have an annoying issue where, if you use “Save As…” with the “Save as type: PDF” option, they convert all instances of Helvetica to Arial. To get around this, use the “Print…” menu and choose “Microsoft Print to PDF” (or similar options from Foxit or Adobe if you have them installed) as the printer. (Not an issue on Macs.) "],["reliability-and-open-science.html", "Section 8 Reliability and Open Science 8.1 Approaches to promote reliability 8.2 Open science", " Section 8 Reliability and Open Science Sadly, it has become clear that many published scientific studies fail to replicate–that is, other scientists are unable to produce the same results, such that reported findings cannot be relied upon by others (and, therefore, that these “findings” do not represent a genuine contribution to human knowledge). While this problem partly reflects inherent challenges in scientific discovery, repeated and pervasive failures of reproducibility call the value of science into question (see Organizing Principles). Lack of reproducibility has many sources, which we should understand so as to minimize their influence in our work (and also to avoid being misled when reading papers by other labs): Dependence on unreliable methods. This includes “p-hacking” (abuse of excessive researcher degrees of freedom in study methods) “HARKing” (hypothesizing after results are known), and widespread misunderstandings of statistical approaches appropriate to exploratory as opposed to confirmatory analyses; also, use of underpowered study designs and related confusions about the difference between power and positive predictive value. For more backfground see Simmons, Nelson &amp; Simonsohn 2011, Button et al 2013, and Ioannidis 2005 in the Resources\\Papers-methods-and-reliability folder of the R: drive. Human error. Every time someone touches or manipulates a dataset, this introduces the possibility of unintended errors. We expect human beings to make errors, so we have to be vigilant about checking our own work and asking other members of our team to help check our work. This means that we have to use methods that make it possible to identify and diagnose errors, such as by tracking and logging transformations of data. This is one reason that we strongly prefer that manipulations be performed in code, rather than manually in programs like Excel (see again Deprecated/discouraged tools in Section 3). We also always want to preserve and archive our raw, unmanipulated datasets so that we can always start over if absolutely necessary. “Bad” luck. Suppose that 20 different research teams are investigating the same false hypothesis. Even if the null hypothesis holds (there is no true finding), given a conventional significance value of 0.05 it is likely that one of these teams will reach a statistically significant result by chance. Given a publishing bias for positive over negative studies (see point 4 below), this false finding is likely to be published while the 19 correct, negative studies would be ignored. This is often a hollow accomplishment, as labs tend to base future studies on previous findings, so publishing a paper based on a false chance finding can lead to years of wasted work pursuing scientific dead ends. Perverse incentives in science. Researchers face tremendous career pressures to publish in order to keep their jobs or secure funding, and publishers generally favor positive over negative results and “surprising” findings over those that follow more directly from what is already known. All of these incentives promote the production and dissemination of false positive findings. Finally, there’s one more sense of “reproducibility” that is important for the organization of our lab. As emphasized earlier, most members of the lab only stay for 2-3 years, so much of the work you’re doing now will be completed by future members. You should try to document your work in a way that it will be intelligible to future lab members, who can then pick up your work without needing to rebuild your analyses from scratch. 8.1 Approaches to promote reliability There are some broad principles and practices that we use in the lab to promote the reliability of our work, that everyone (particularly those engaged in quantitative research) should understand: Distinguishing exploratory vs. hypothesis-driven design. One of the major problems in quantitative research is that many scientists’ aims are in fact exploratory (e.g., trying to look for new things and elucidate processes that haven’t yet been discovered), but our accepted statistical thresholds for publication (e.g., p values and confidence intervals) implicitly assume confirmatory (hypothesis-driven) methods. So many scientists, often without really understanding the statistical issues involved, report exploratory work as if it were hypothesis-driven (e.g., reporting on p values) even though the hypotheses being tested were not actually formulated prior to looking at the data. This violates the assumptions of significance testing and so often contributes to false positive reports. Unfortunately, how to properly report exploratory research remains an open question that the scientific community is still working out. Still, we should remain careful in our thinking about when our aims are exploratory and when they’re hypothesis-driven, and we should not try to pass off exploratory work as hypothesis-driven. Study pre-registration when feasible; in all cases, pre-planning. In general, if we are conducting a hypothesis-driven study, it makes good sense to pre-register our study design. Generally we use the Open Science Framework. Pre-registration has been standard practice in randomized controlled trials for many years, and is now being extended to other kinds of experimental research. Even when we are not pre-registering studies or we are planning purely exploratory work, it is generally a useful exercise to think in advance about what questions we are interested in, and what would be the best approaches to answer these questions, before we start poking around in datasets. Hopefully this can help us avoid treating noise in our dataset as signal. Documentation. This is something I struggle with, but is crucially important for other members of the lab and also for yourself. It’s natural to assume, when you’re totally cognitively engaged in a problem, that it will be very easy for you to recall later what you did and why. In fact we all tend to overestimate our recall for these details, once we move on to solve new problems. It’s really important to use our electronic lab notebooks in the private Github repo to document our thought processes. (This will help you in the future as well.) This is particularly crucial for recording a paper trail of analytic decisions, data cleaning, and other decisions affecting findings (e.g., leaving out outliers and why). “Literate programming.” This refers to a different way of thinking about work that we do in code. As expressed by the computer scientist Donald Knuth: “Let us change our traditional attitude to the construction of programs. Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to humans what we want the computer to do.” In other words, think of your code (whether in R, Stata, Matlab, or any other language) as like an essay intended to communicate to other human beings. This should follow the order and logic of human thought, and not just the forms imposed by the computer. It is not only important that the code “works” (in that it makes the computer perform the operations that you intend); it’s also important that people who read your scripts (including your future self) can understand the flow of your thinking and what each piece of code is intended to do. Section 9: RStudio Analysis Pathway provides a detailed tutorial to one implementation of literate programming. Publishing our code. Increasingly, it will be an expectation that when someone publishes a scientific study, that they also make their datasets and code publicly accessible so that other scientists can check their work. After all, if you’re not confident enough in your code to allow other scientists to review it, then should you be confident enough to publish findings based on this code? If you’re reading a study but the authors are not willing to publish their code, should you believe what they report? So as noted in Section 3.7: Open Science Framework, we are publicly posting our code when we publish papers. So, this is another reason to work hard to keep your code clean and readable–we expect reviewers, editors, and readers to be looking at it. Coming soon: code review. As above, human error is common and expected. We should all be diligent about checking our own work, but in most cases we also need other people to help us find errors that we can’t find ourselves. Software developers don’t rely solely on individual programmers to find errors in their code, but instead use collaborative tools so that teams can check one another. This is one of the reasons why all of our code is intended to be shared in the Github repository, rather than just being distributed across different people’s desktop computers. In the coming year, I’m hoping to start using collaborative tools in Github that facilitate review of our code by other lab members. This is another reason why it’s important to write “literate code” that can be readily understood by others, and not just a series of computational operations. 8.2 Open science The term “open science” is used in many different ways by different scientists, but in general the ideas above follow many of the broad principles of open science. We want to produce work that can be relied upon, used, and extended by other scientists. When feasible, we will pre-register studies, particularly for hypothesis-driven work. We will also aim to post our code when we publish our studies, in part to encourage ourselves to check our code carefully. When permitted by publishers’ policies, we will post free versions of our papers on our lab website so that scholars everywhere can read them. One respect in which our science may not be fully open has to do with the openness of our datasets. Much of our work involves stigmatized disorders such as dementia, or potentially sensitive topics such as money management or vulnerability to financial errors in aging. For published studies, we should review the consent forms for particular studies as well as MAC data sharing policies to see what forms of data sharing are consistent with center policies and participants’ reasonable expectations regarding the privacy of potentially sensitive data. When feasible and ethically permissible, we should aim to share our datasets (potentially also in redacted form). "],["rstudio-analysis-pathway.html", "Section 9 RStudio Analysis Pathway 9.1 Basic overview of the pathway 9.2 Step 0: Review software tools 9.3 Step 1: Clone the decisionlab repository to your computer 9.4 Step 2: Save the original dataset to the R: drive 9.5 Step 3: Clean the data in RStudio using an RMarkdown (.Rmd) and creating an .html log, creating a new .Rda data file on the R: drive 9.6 Step 4: Analyze the data using other .Rmd files (saved in GitHub), generating more .html logs (in GitHub) and more .Rda files as necessary on the R: drive", " Section 9 RStudio Analysis Pathway For lab members using quantitative methods (e.g., neuroscience); not critical but potentially of interest for those working in neuroethics/qualitative methods This analysis pathway represents one example of our preferred approach to quantitative code and data handling. This pathway has several key aims intended to promote reliable scientific practices (see Reliability and Open Science): Ensure the integrity of the dataset. For this reason, after the dataset has been imported from Qualtrics or e-Prime, we never touch the original dataset. No cutting/copying/pasting in Excel! Document all the steps that go into a paper or poster. If we exclude a subject, recode a variable or create a new variable, etc. we want to be able to say exactly what we’ve done. Keep the ability to repeat everything easily and accurately. For instance, what if… You lose or accidentally write over your data files? (If your source data is still in Qualtrics, can you get back to where you were before without starting from scratch?) You collect more data? For instance, you submit a paper with 50 subjects from a Qualtrics instrument and your reviewers ask for a larger sample. So you collect another 50 subjects with the Qualtrics instrument. Can you download your new enlarged dataset and re-run your analyses without reinventing the wheel? Utilize literate programming to explain to other lab members and future reviewers what each step is intended to accomplish. 9.1 Basic overview of the pathway Review software tools: R, RStudio, GitHub, SourceTree Clone the decisionlab repo to your computer (if you haven’t already) Save original dataset to the R: drive Clean the data with an RMarkdown (.Rmd) saved in GitHub, creating an .html log that is saved to GitHub and a new .Rda data file on the R: drive Analyze the data using other .Rmd files (saved in GitHub), generating more .html logs (in GitHub) and more .Rda files as necessary on the R: drive 9.1.1 Other resources For more background info, start of course with the lab private repository (decisionlab) README.md file. There is an example data cleaning file in AgingCog/analysis/moral-dilemma/moral-dilemma_01_clean-merge-split.Rmd. This pathway is based on the use of .do and .log files in Stata; for more info on that see the Powerpoint “Lecture 2” in R:/groups/chiong/Resources/Biostat_212. Also, for file naming, variable naming, and other conventions, we will try to follow the Tidyverse Style Guide at style.tidyverse.org. There are also some useful suggestions about file naming in Naming Things and in the OSF Best Practices File Naming Guide. 9.2 Step 0: Review software tools 9.2.1 R R is a free programming language and statistical/graphics environment, similar to programs like SAS, SPSS, Stata or Matlab. While R is in many respects harder to learn than many of those tools, it’s very useful to know because it’s becoming more widely used in a variety of fields. Because it’s free, it’s more commonly used in undergraduate teaching, and in resource-constrained (e.g. nonprofit or international) environments. Also, because it’s free, use of R is most consistent with principles of open science. That is, if you post a dataset and code that requires paid software like Stata or Matlab, then only people who already own those programs can download and use what you share. Whereas, if you post a dataset and code in R, then anyone in the world can download and use what you share, because they can also download and use R for free. For these reasons, there’s now a very robust and committed user base of people who contribute to and improve R, and R now generally equals or exceeds paid software in reliability and capabilities. Download and install R from www.r-project.org. There are a lot of great online tools for learning R–here are a few that we’ve found useful and that utilize R in similar ways as we plan to in lab: Statistical Thinking for the 21st Century - stats textbook by Russ Poldrack with R companion R for Data Science - a general resource for thinking about working with data Functional Programming - chapters 1-3 introduce different data structures used in R, later chapters explain functional programming (a more advanced, very useful tool) R Graphics Cookbook - how to generate high-quality graphics quickly using ggplot2 ggplot2: Elegant Graphics for Data Analysis - more intro/background on theory and how ggplot2 works Cheatsheets! dplyr - tidyverse data transformation ggplot2 - data visualization R Markdown readr/tidyr - data import and tibbbles Base R Stata to R 9.2.2 RStudio RStudio is an open source interface for R that makes it much more user-friendly, and (crucial for our purposes) implements R Markdown, which allows us to create .html logs that preserve and communicate details of our analyses. After installing R, install RStudio from www.rstudio.com/products/rstudio/download/. Let’s play a little bit with RStudio and R Markdown. (There’s also a useful quick tour at rmarkdown.rstudio.com/authoring_quick_tour.html.) Open RStudio and go to “File &gt; New File &gt; New R Markdown…”. Give the new file a title (“Untitled” is fine for now), put in your name, and select HTML as the default output format. Now you’ll see an RStudio screen with four windows: The top left window is the source window, used for editing code. Here we’re editing an R Markdown (.Rmd) file. (If you’re familiar with Stata, this is like the Script Editor.) The bottom left window is the console. This allows you to run quick operations that you’re not necessarily planning to save in your code file. You can type right next to the &gt; character. Type 1 and press Enter–this will return the output [1] 1. Now type 4+3 and press Enter–this should return the output [1] 7. The upper right window shows your workspace. (It can also be used to review the history of commands.) A nice thing about R is that you can use and manipulate many objects in your workspace (datasets, variables, etc.) at the same time. Back in the console, type a &lt;- 4+3 and press Enter. (“&lt;-” is the assignment operator in R.) Now in the upper right window you should see a and its value, 7. You can now perform operations with this variable: in the console, type sqrt(a) and press Enter. Or, you can store these outputs in new variables: type b &lt;- sqrt(a) and press Enter. Finally, the bottom right window shows you the file structure, graphical output, available packages, and help documents. Look more closely at the source window at top left. If you look at the top, you may recognize this as an ordinary YAML header, similar to those we use to post news on our lab website. If you scroll through the starter R Markdown file provided, you’ll notice that some sections of the file have a white background, alternating with sections that have a grey background and are preceded and followed by 3 “backticks” that look like this: ```. This is an implementation of literate programming: the sections in white are ordinary Markdown, which is used to explain one’s reasoning to the reader, and the sections in grey are “code chunks” that contain instructions for the computer. (The first line of a code chunk will have 3 backticks and then curly brackets enclosing r indicating that the code is in R language, then optionally a name for the code chunk, then optionally a comma followed by special parameters for the code chunk.) To make RStudio use your code, you have two options: run and knit. Often when you’re working on your code you’ll use run. To explore options, pull down the menu for “Run” at the top of the source window: Now click inside the source window and navigate so that your cursor is inside of the second code chunk (named “cars”)–that is, on on lines 18-20. Pull down on “Run” and select Run Current Chunk”. Look in the console window at bottom left–there you’ll see that RStudio behaves as if you had typed summary(cars) into the console, and displays the output. In the source window at top left, the chunk output is also displayed immediately below the code chunk, inserted between lines 20 and 21. You can click on ︽ to collapse this output, or on x to clear it. You can try the same with graphics: click inside the third code chunk named “pressure,” on lines 26-28, and select “Run Current Chunk” again. This should display a plot in the source window. When your code is complete and ready to be officially run and timestamped, you will use knit to create the .html log of your code. Select “Knit” from the menu above the source window (to the right of the “Run” menu). This will open a dialog box asking you where to save the files you’re creating–for this one, don’t save to the repository (this is a test, other people don’t need these files). Just pick somewhere else on your hard drive and give a temporary name. Now you’ll see a nicely-formatted .html file with your formatted Markdown and the outputs of your R code. Okay, two more quick tweaks. First, RStudio tends to want to autosave your workspace, which can be convenient for someone just working on their own computer but a potential source of confusion when multiple people are working collaboratively across multiple computers. Go to “Tools &gt; Global Options… &gt; General” and uncheck “Restore most recently opened project at startup,” “Restore previously open source documents at startup,” and “Restore .RData into workspace at startup,” and change “Save workspace to .RData on exit:” to “Never.” Also, we will try to keep the line length of our code less than 80 characters. Go to “Tools &gt; Global Options… &gt; Code &gt; Display” and check the box for “Show margin” with “margin column” set to 80. This will draw a reminder line. When your code lines overrun the 80 character limit, break them up into multiple lines (if feasible) and use spaces to align them. 9.2.3 GitHub Some of the uses of GitHub in our lab have been explained already in the decisionlabucsf website README, the private decisionlab repo README, and in this handbook Section 3: Lab Resources Overview. Like Google Docs, GitHub allows multiple people to work collaboratively on the same piece of code, keeping track of who made what changes and when, making it easy to reverse changes that have been made, and allowing different versions to be developed at the same time. 9.2.4 SourceTree Up until now, you might have used GitHub directly thouugh the GitHub website at www.github.com –e.g., to update your lab profile or to update one of the lab notebooks. While this is fine for editing Markdown and YAML files, now we want to use our private GitHub repository to store scripts and code for use in RStudio. (Also for other packages like Matlab, Stata, etc.) RStudio can’t read files from the github.com website directly, so we’re going to use SourceTree to sync our repository to a local folder on your computer. Go to https://www.sourcetreeapp.com to download the app. (You might need to sign up for a free “Bitbucket” account to do this, but hopefully not.) The instructions on the Sourcetree website are honestly not super-helpful. For our purposes, go to Preferences -&gt; Accounts… or Tools -&gt; Options -&gt; Authentication -&gt; Accounts (depending on what kind of computer you have) and choose “Add.” For Host enter “GitHub”, for Preferred Protocol enter “HTTPS”, and for Authentication enter “OAuth”. Then click on “Refresh OAuth Token”. A window will open and load in your browser to log into GitHub. You need to log in and then click “Authorize atlassian” to authorize SourceTree to access your GitHub account. We’ll discuss using SourceTree in more detail in Step 1. One quick note here: we do want to be really careful with pulling (which will rewrite the contents of your local folder to match the GitHub repository) and pushing (which will rewrite the contents of the GitHub repository to match your local folder). So please pay attention to what SourceTree is doing here: if you’re trying to push changes to just one file but you notice that SourceTree is planning to make changes to a hundred files, slow down–that might be a sign that something has gone wrong. If you’re not sure what is happening, feel free to send me a question in Slack! 9.3 Step 1: Clone the decisionlab repository to your computer In this pathway, we want RStudio to deal with files in two different places. The data files all stay on the R: drive, but the .Rmd and .html files go in GitHub. I’m assuming you already have the R: drive mounted, so now let’s use SourceTree to get a copy of the repository on your computer. When you create this copy, this is called “cloning” a remote repository. Create a new local folder on your computer. To keep our scripts consistent, call this “decisionlab” and put it in your user directory. That is, if you’re using a PC this new folder should be C:\\Users\\[YOURNAME]\\decisionlab (where [YOURNAME] is your username on the computer), and if you’re using a Mac this new folder should be Users/[YOURNAME]/decisionlab. Now obtain the URL for cloning the repository from the GitHub website (remember, you want the private “UCSFMemoryAndAging/decisionlab” repo here, not the public “decisionlabucsf” repo that we use for the website) by finding the green button for “&lt; &gt; Code” and copying the URL for the repository: Now open SourceTree, and go to File &gt; Clone/New… This will open a window as seen below. For “Source Path/URL:” paste in the URL from GitHub for cloning. For “Destination Path:” give the address of the local folder you’ve just created. When ready, press “Clone.” (Also–at some point in this process you may get a pop-up labeled CredentialHelperSelector asking you to select a credential helper. I don’t really know what this is, but I chose “manager” and “Always use this from now on” and it seems to be working fine. Based on some discussions online it sounds like “manager-core” should work also.) Now you’ll have a local copy of the repository on your computer, and everyone else in lab will have a local copy of the repository on their computers. (Another reason why we don’t want to put huge files in the repository–they will clog up everyone’s hard drives and make syncing really slow for everyone. So keep the files light, no Word or Powerpoint files or datasets.) From now on, when you want to work with the files in the repository, you will: Start SourceTree Pull: get the most recent version of the files on your computer (otherwise if someone else has been editing the file since the last time you synced your local repo to the remote shared repo, you’re going to create a conflict) Do your work in RStudio, and save updated or new local copies of the files to your local repo folder Commit: prepare any changed files in your local repository (e.g., C:\\Users\\[YOURNAME]\\decisionlab) to go in the shared repository, including a quick description of what you’ve done (“staging”) Pull: check again that you’re totally up to date with the online files (e.g., in case other people have also been working on them without your knowledge) Push: update the shared remote repository to include the new changes you’ve made, so now everything in local repository and the online repository should match again To help you remember, when you look at the top of SourceTree, you’ll see the last 3 operations in order: Commit-Pull-Push. This is what you always do when saving your work to GitHub: Now that you’ve set up your local repo, let’s take a look at it and the R: drive. Let’s suppose you’ve just collected data for a new task we’ll just call “blah.” Right now, because this is the first time you’ll be looking at the data, you make a new folder for your data on the R: drive and create a new folder for “blah” inside your local GitHub repo: 9.4 Step 2: Save the original dataset to the R: drive Most of our data will be collected in e-Prime or in Qualtrics. Save the file, e.g. directly from Qualtrics, in your new folder on the R: drive. Usually this will either be in CSV or Excel format. (Qualtrics will export the CSV file using its own ugly naming convention, which you don’t need to change.) From now on we only read this data file. We don’t save any changes to it. If we ever had to re-download the data file from Qualtrics to the R: drive again (e.g., if there was some crash or a data loss), we could reproduce every step in the analysis again, regardless of what we do from this point forward. 9.5 Step 3: Clean the data in RStudio using an RMarkdown (.Rmd) and creating an .html log, creating a new .Rda data file on the R: drive Because we’re not going to save future changes to this CSV file, we’re going to use R code in an .Rmd file to create a clean data file in R format (.Rda). When this is complete, knitting this .Rmd file will run all of the embedded R code (so, rewriting the .Rda file if you have preliminary versions you created while editing the code) and create documentation in the form of an .html log. The .Rmd and .html files go in your local copy of the repository (to be pushed to GitHub by SourceTree), and the .Rda file goes on the R: drive. (This is a little contrary to how RStudio likes to work, which is having the code and data files in the same directory, but is necessary for us to preserve the confidentiality of our participant data.) As an example of a data cleaning file, see AgingCog/analysis/moral-dilemma/moral-dilemma_01_clean-merge-split.Rmd. At the top you’ll see two code chunks called “timestamp” and “setup,” a version of which we probably need in every R Markdown file that we run for the lab. When you create your own .Rmd file you’ll use “timestamp” to run the R command Sys.time() to note when the file is knit and write this into the .html log. When you create your own .Rmd file, you’ll use “setup” to tell RStudio which folders we want to use for the “repofolder” (the folder in the GitHub repository where we’re putting our .Rmd and .html files) and “datafolder” (the folder on the R: drive where the .csv and .RDa files will go). You’ll need to check these values twice, because we want the code to run regardless of whether someone else is doing this on a Windows machine or a Mac; the Windows code also includes a workaround to account for the fact that people have different usernames. This also includes an address for “rdrivepath” in case we want to reference other things on the R: drive–this points to different paths on Windows and Mac machines. Finally, the “setup” chunk will be used to load any special libraries that RStudio needs to run the code. For our hypothetical example for task “blah”: ```{r setup} if (Sys.info()[&quot;sysname&quot;] == &quot;Windows&quot;){ repofolder &lt;- paste(&quot;C:/Users/&quot;, Sys.getenv(&quot;USERNAME&quot;), &quot;/decisionlab/blah/analysis/&quot;, sep = &quot;&quot;) datafolder &lt;- &quot;R:/groups/chiong/blahblah/blah/&quot; rdrivepath &lt;- &quot;R:/&quot; } else if (Sys.info()[&quot;sysname&quot;] == &quot;Darwin&quot;){ # not sure why MacOS is &quot;Darwin&quot; but it is... repofolder &lt;- &quot;~/decisionlab/blah/analysis/&quot; datafolder &lt;- &quot;/Volumes/macdata/groups/chiong/blahblah/blah/&quot; rdrivepath &lt;- &quot;/Volumes/macdata/&quot; } # probably good to add another condition for Unix, later on... library(readr) library(readxl) knitr::opts_chunk$set(echo = TRUE) knitr::opts_knit$set(root.dir = datafolder) ``` Before we get too far along, let’s save the .Rmd file. Again, this goes within the decisionlab repo local folder on your computer, which you created in Step 1. (From where it can then be pushed to the lab GitHub repo.) Now you’ll have two files: your original data file on the R: drive, and the .Rmd file that you’re currently working on in your local repo: Note that, since you haven’t actually knit this file yet, you haven’t created an .html file. Select “Knit” from the top of the source window. You’ll see some output run by in an “R Markdown” tab of the console window, and then R will display an .html file based upon your R Markdown file, displaying the two code blocks we have so far: Note the white box in which it displays the output from Sys.time()–this is a timestamp of exactly when the .html was knit from the .Rmd file. This .html file will also show up in your local repo subfolder, preserving a log of what you’ve done: Okay, let’s keep working on your .Rmd file. Again, see the example file for moral-dilemma in AgingCog for an example. In general, what you’ll do is test out code in the console and check that the code does what you want it to do (e.g., by reviewing the variables and datasets created in the environment), and then copy the working code that you want to keep into code chunks into your .Rmd file in the source window. But there’s one finicky thing about directories. In general, R always executes commands relative to a “working directory”–that’s the folder where it’s reading and writing files. When R Markdown knits your html file, the code above includes knitr::opts_knit$set(root.dir = datafolder)–this tells it to use “datafolder” as the default directory for reading and writing files. But when you’re working interactively in the console and using run to test your code chunks, it doesn’t see that code and know to look in “datafolder.” So, you’ll need to do 3 things: Run your “setup” code chunk by clicking in the source window at upper left, navigating to place your cursor somewhere inside the setup chunk, and selecting “Run Current Chunk.” In your workspace at upper right, this should create values for “datafolder,” “rdrivepath,” and “repofolder” with Windows locations if you’re in Windows and with Mac locations if you’re working on a Mac. In the console window at bottom left, type setwd(datafolder) and press Enter. This will change your current working directory to “datafolder” to match your R Markdown script. Just to check that you’ve done this correctly, in the panel at bottom right select “More &gt; Go To Working Directory.” This should show you the datafolder that you’ve specified. The next step is to import the Qualtrics data into RStudio so that you can start cleaning the dataset. Here, you’ll use the GUI in RStudio to perform some operations, for which the corresponding code will show up in the console window. Now the panel at bottom right should show your datafolder, which includes the CSV file that you imported from Qualtrics. Click on this file and select “Import Dataset…” This will open a new window with options for importing this dataset, and a handy preview of what the dataset will look like with those options once imported. More importantly, on the bottom right is a “Code Preview” window–this shows you what you would need to write in R code in order to perform the same operation. Also, there’s a clipboard icon that will copy out this code for you to use: Change the “Name” to something more reasonable, like “blah_qualtrics_df.” (The suffix “_df” is a useful convention for marking an object in the workspace as a dataframe.) You will notice (you might need to click out of the text field, so that it updates) that the code in the “Code Preview” window changes accordingly. Also, uncheck the option for “Open Data Viewer.” You could now just click “Import” to import the dataset through the GUI. However, for our purposes we’re mainly interested in generating the code that will import the data, rather than just importing the data. So click on the clipboard icon to copy the code, and then click “Cancel” to exit this window. Now you can paste this code into the console window to make sure that the code works as intended. Press Enter, and you should see the dataframe “blah_qualtrics_df” in your workspace: If you like the code and want to keep it, paste it into the R Markdown file. Below the “setup” code chunk, start a new section with a header such as “Data cleaning.” After this you can write a description of what the next code chunk is supposed to do, and you can create a new code chunk–call it “file-import”: There will be a lot of other cleaning to do before the data can be analyzed. For instance, when Qualtrics exports CSV files the first three rows are different versions of the header (info about the columns), but R will assume that the first row is the column name and rows 2 and 3 are your first two observations. So you’ll need to extract the column info you want, and then drop the first two rows because they’re not observations. You’ll also want to convert any date strings in the dataframe to proper R dates. So soon this part of your R Markdown file will look like this: ## Data cleaning First, load data from .csv file. Then drop first two rows from Qualtrics, and convert Qualtrics datstrings to R dates. ```{r file-import} library(readr) blah_qualtrics_df &lt;- read_csv(&quot;ugly_Qualtrics_filename_Date_Time.csv&quot;) blah_qualtrics_df &lt;- blah_qualtrics_df[-c(1,2),] blah_qualtrics_df$StartDate &lt;- as.POSIXct(blah_qualtrics_df$StartDate, format = &quot;%Y-%m-%d %H:%M&quot;) blah_qualtrics_df$EndDate &lt;- as.POSIXct(blah_qualtrics_df$EndDate, format = &quot;%Y-%m-%d %H:%M&quot;) blah_qualtrics_df$RecordedDate &lt;- as.POSIXct(blah_qualtrics_df$RecordedDate, format = &quot;%Y-%m-%d %H:%M&quot;) ``` You may also need to drop other observations (e.g., patients we collected data on who don’t have the diagnoses of interest, subjects who failed control conditions… though it might be better to create a new variable named something like “badobs” with 1s and 0s, which you can just use to exclude subjects from your analysis later on rather than delete entirely), recode variables, create new variables, etc. You probably won’t finish the code for data cleaning all in one sitting! When you’re done working for now, to save the work you’ve done so far you need to save your .Rmd file to the local repository folder. Then, to put this file (and your .html file if you’ve knit it) on the online shared repo, go back to SourceTree and commit-pull-push. When it’s time to come back and work on the files, pull using SourceTree to make sure that you have the most recent version of the files on your local machine, and then open the .Rmd file again in RStudio. When you’re done with the cleaning code, your .Rmd file needs to tell RStudio to create a new .Rda file that incorporates all of your changes, in the R: drive: ## Save dataset in R format ```{r save-rda} save(blah_qualtrics_df, file=&quot;blah_clean_qualtrics_df.Rda&quot;) ``` Now when you knit the .Rmd file, it will create a new .Rda file on the R: drive, and will also re-write the .html file to your local repository in your user folder. (This is good, now you also have a timestamp of when the .Rda file on the R: drive was most recently created.) Don’t forget to save your completed .Rmd file, and commit-pull-push to get your updated .Rmd and .html files on to the shared GitHub repository. 9.6 Step 4: Analyze the data using other .Rmd files (saved in GitHub), generating more .html logs (in GitHub) and more .Rda files as necessary on the R: drive Again, once you’ve created a clean dataset, you won’t touch the original .csv or Excel data file in the R: drive anymore. Instead, you’ll create some new .Rmd files in your local GitHub repository that read the data from the clean dataset. We always want to be able to retrace our steps, so if these new .Rmd files include manipulations of the data that we want to keep (like parameter estimates from a regression), then we save these in a new .Rda file on the R: drive. For instance: As a general principle, if an R Markdown script reads data from a given .Rda file, then that .Rmd should not also save changes to that same .Rda file. Instead, if there are any manipulations made to the data that need to be saved, then these should be saved in a new .Rda file. (In other words, if we’re drawing a map with arrows like the one above, showing the dependencies between .Rda and .Rmd files, there should be no “closed loops”–the flow between any two files should always be in only one direction.) Otherwise, we may not be able to retrace our steps if we need to reconstruct any of these files. (At this point, it might also be helpful to create a README.md file in the local GitHub repository, explaining what order the R Markdown scritps should be knit in to re-create the data sets, and how all the files are related to one another.) Just to complete the illustration, here’s a hypothetical later stage of the project: For these later stages (creating the tables and figures), there are no manipulations of the data that need to be recorded, so there’s no need to create more .Rda files. In the first stage (in purple), we read out the original .Rda file to make some tables. The table output gets written in the .html file, and we don’t need to create new files on the R: drive. In the second stage (in orange), we use some of the created variables from our analysis (e.g., parameters from a regression) to make a figure. This .Rmd file generates its own .html file, and we save a vector image (see again More about figures in Section 7 on Authorship) of the figure in the R: drive (to avoid cluttering the GitHub repo). 9.6.1 Some observations Note now that: All datasets with PHI and identifiers (names, demographics, etc.) stay on the R: drive. All big files, including datasets and graphic files, stay on the R: drive. Worst case scenario: you accidentally overwrite files on the R: drive, or even the R: drive gets wiped out. You can still recover every step of your analysis. The original data file csv can be downloaded again from Qualtrics. GitHub keeps timestamped versions of every file in the online repository. All the information you need to reconstruct the .Rda and graphics files on the R: drive are contained in the .Rmd files in the GitHub repository. What if you collect more subjects, e.g. if journal reviewers ask for a larger sample? Download the new, csv export file from Qualtrics. Edit and rename the data cleaning .Rmd file that you made before, and inside this new .Rmd file create new filenames for the .html and .Rda files that are created. Now you can reliably reproduce all of the steps you used before on your new, expanded dataset. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
